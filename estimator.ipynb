{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import holidays\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import StackingRegressor, AdaBoostRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LassoLarsCV, ElasticNetCV, RidgeCV, LassoCV, SGDRegressor\n",
    "import numpy as np\n",
    "import problem\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "#!pip install category_encoders\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train data\n",
    "X, y = problem.get_train_data()\n",
    "external_data = pd.read_csv(os.path.join('submissions', 'test_final', 'external_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateOfDeparture              object\n",
       "event_level_dep             float64\n",
       "mean_temp_dep                 int64\n",
       "year                          int64\n",
       "month                         int64\n",
       "Departure                    object\n",
       "Arrival                      object\n",
       "departures_performed        float64\n",
       "passengers                  float64\n",
       "utilization_rate            float64\n",
       "distance                    float64\n",
       "passengers_per_departure    float64\n",
       "event_level_arr             float64\n",
       "gdp_capita_dep              float64\n",
       "population_dep                int64\n",
       "gdp_capita_arr              float64\n",
       "population_arr                int64\n",
       "sum_gdp_dep_arr             float64\n",
       "sum_pop_dep_arr               int64\n",
       "avg_monthly_cost_gallon     float64\n",
       "perc_cancelled              float64\n",
       "is_holiday                     bool\n",
       "holidays_distance             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "external_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateOfDeparture</th>\n",
       "      <th>event_level_dep</th>\n",
       "      <th>mean_temp_dep</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>Departure</th>\n",
       "      <th>Arrival</th>\n",
       "      <th>departures_performed</th>\n",
       "      <th>passengers</th>\n",
       "      <th>utilization_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>gdp_capita_dep</th>\n",
       "      <th>population_dep</th>\n",
       "      <th>gdp_capita_arr</th>\n",
       "      <th>population_arr</th>\n",
       "      <th>sum_gdp_dep_arr</th>\n",
       "      <th>sum_pop_dep_arr</th>\n",
       "      <th>avg_monthly_cost_gallon</th>\n",
       "      <th>perc_cancelled</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>holidays_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-09-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29</td>\n",
       "      <td>2011</td>\n",
       "      <td>9</td>\n",
       "      <td>ATL</td>\n",
       "      <td>BOS</td>\n",
       "      <td>465.0</td>\n",
       "      <td>61995.0</td>\n",
       "      <td>0.864427</td>\n",
       "      <td>...</td>\n",
       "      <td>56783.0</td>\n",
       "      <td>437812</td>\n",
       "      <td>81680.0</td>\n",
       "      <td>630505</td>\n",
       "      <td>138463.0</td>\n",
       "      <td>1068317</td>\n",
       "      <td>0.3087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-09-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24</td>\n",
       "      <td>2011</td>\n",
       "      <td>9</td>\n",
       "      <td>EWR</td>\n",
       "      <td>IAH</td>\n",
       "      <td>303.0</td>\n",
       "      <td>40906.0</td>\n",
       "      <td>0.735640</td>\n",
       "      <td>...</td>\n",
       "      <td>72237.0</td>\n",
       "      <td>8272948</td>\n",
       "      <td>66407.0</td>\n",
       "      <td>2126032</td>\n",
       "      <td>138644.0</td>\n",
       "      <td>10398980</td>\n",
       "      <td>0.3087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-09-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28</td>\n",
       "      <td>2011</td>\n",
       "      <td>9</td>\n",
       "      <td>DEN</td>\n",
       "      <td>LAS</td>\n",
       "      <td>599.0</td>\n",
       "      <td>77612.0</td>\n",
       "      <td>0.909263</td>\n",
       "      <td>...</td>\n",
       "      <td>59659.0</td>\n",
       "      <td>620530</td>\n",
       "      <td>47641.0</td>\n",
       "      <td>586606</td>\n",
       "      <td>107300.0</td>\n",
       "      <td>1207136</td>\n",
       "      <td>0.3087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-09-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24</td>\n",
       "      <td>2011</td>\n",
       "      <td>9</td>\n",
       "      <td>EWR</td>\n",
       "      <td>DEN</td>\n",
       "      <td>257.0</td>\n",
       "      <td>32136.0</td>\n",
       "      <td>0.821577</td>\n",
       "      <td>...</td>\n",
       "      <td>72237.0</td>\n",
       "      <td>8272948</td>\n",
       "      <td>59659.0</td>\n",
       "      <td>620530</td>\n",
       "      <td>131896.0</td>\n",
       "      <td>8893478</td>\n",
       "      <td>0.3087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-09-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24</td>\n",
       "      <td>2011</td>\n",
       "      <td>9</td>\n",
       "      <td>EWR</td>\n",
       "      <td>CLT</td>\n",
       "      <td>440.0</td>\n",
       "      <td>38418.0</td>\n",
       "      <td>0.783641</td>\n",
       "      <td>...</td>\n",
       "      <td>72237.0</td>\n",
       "      <td>8272948</td>\n",
       "      <td>61277.0</td>\n",
       "      <td>754829</td>\n",
       "      <td>133514.0</td>\n",
       "      <td>9027777</td>\n",
       "      <td>0.3087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  DateOfDeparture  event_level_dep  mean_temp_dep  year  month Departure  \\\n",
       "0      2011-09-01              0.0             29  2011      9       ATL   \n",
       "1      2011-09-01              0.0             24  2011      9       EWR   \n",
       "2      2011-09-01              0.0             28  2011      9       DEN   \n",
       "3      2011-09-01              0.0             24  2011      9       EWR   \n",
       "4      2011-09-01              0.0             24  2011      9       EWR   \n",
       "\n",
       "  Arrival  departures_performed  passengers  utilization_rate  ...  \\\n",
       "0     BOS                 465.0     61995.0          0.864427  ...   \n",
       "1     IAH                 303.0     40906.0          0.735640  ...   \n",
       "2     LAS                 599.0     77612.0          0.909263  ...   \n",
       "3     DEN                 257.0     32136.0          0.821577  ...   \n",
       "4     CLT                 440.0     38418.0          0.783641  ...   \n",
       "\n",
       "   gdp_capita_dep  population_dep  gdp_capita_arr  population_arr  \\\n",
       "0         56783.0          437812         81680.0          630505   \n",
       "1         72237.0         8272948         66407.0         2126032   \n",
       "2         59659.0          620530         47641.0          586606   \n",
       "3         72237.0         8272948         59659.0          620530   \n",
       "4         72237.0         8272948         61277.0          754829   \n",
       "\n",
       "   sum_gdp_dep_arr  sum_pop_dep_arr  avg_monthly_cost_gallon  perc_cancelled  \\\n",
       "0         138463.0          1068317                   0.3087             0.0   \n",
       "1         138644.0         10398980                   0.3087             0.0   \n",
       "2         107300.0          1207136                   0.3087             0.0   \n",
       "3         131896.0          8893478                   0.3087             0.0   \n",
       "4         133514.0          9027777                   0.3087             0.0   \n",
       "\n",
       "   is_holiday  holidays_distance  \n",
       "0       False                  4  \n",
       "1       False                  4  \n",
       "2       False                  4  \n",
       "3       False                  4  \n",
       "4       False                  4  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "external_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _merge_external_data(X):\n",
    "    external_data = pd.read_csv(os.path.join('submissions', 'test_final', 'external_data.csv'))\n",
    "    \n",
    "    # create a \"year\" and \"month\" columns to enable the merge\n",
    "    X.loc[:, 'DateOfDeparture'] = pd.to_datetime(X.loc[:, 'DateOfDeparture'])\n",
    "    external_data.loc[:, 'DateOfDeparture'] = pd.to_datetime(external_data.loc[:, 'DateOfDeparture'])\n",
    "    #external_data = external_data.fillna(0.0)\n",
    "    \n",
    "    X['connexion'] = X['Departure'] + '_' + X['Arrival']\n",
    "    X['connexion'] = ['_'.join(np.sort(x.split('_'))) for x in X['connexion']]\n",
    "\n",
    "    external_data['event_level_dep_arr'] = external_data['event_level_dep'] * external_data['event_level_arr']\n",
    "    external_data.drop(columns=['event_level_dep', 'event_level_arr'],inplace=True)\n",
    "\n",
    "    X_merged = X.merge(external_data, how='left', on=['Departure', 'Arrival', 'DateOfDeparture'])\n",
    "    \n",
    "    pass_per_month = external_data[external_data['year'] == 2012].groupby('month').agg('sum')['passengers'].to_dict()\n",
    "    X_merged['month_importance'] = X_merged['month'].replace(pass_per_month)\n",
    "    \n",
    "    pass_per_connexion = X_merged.groupby('connexion').agg('mean')['passengers'].to_dict()\n",
    "    X_merged['connexion_importance'] = X_merged['connexion'].replace(pass_per_connexion)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # super cheat\n",
    "    #X_merged['log_passengers_PAX'] = X_merged['log_passengers'] - y\n",
    "    \n",
    "    X_merged.drop(columns=['mean_temp_dep','passengers_per_departure', 'avg_monthly_cost_gallon'], inplace=True)\n",
    "    return X_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _encode_dates(X):\n",
    "    X_encoded = X.copy()\n",
    "\n",
    "    # Make sure that DateOfDeparture is of datetime format\n",
    "    X_encoded.loc[:, 'DateOfDeparture'] = pd.to_datetime(X_encoded.loc[:, 'DateOfDeparture'])\n",
    "    \n",
    "    # Encode the DateOfDeparture\n",
    "    X_encoded.loc[:, 'day'] = X_encoded['DateOfDeparture'].dt.day\n",
    "    X_encoded.loc[:, 'weekday'] = X_encoded['DateOfDeparture'].dt.weekday\n",
    "    X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
    "    X_encoded.loc[:, 'is_weekend'] = [True if x in [5, 6] else False for x in X_encoded.loc[:, 'weekday']]\n",
    "    X_encoded.loc[:, 'n_days'] = X_encoded['DateOfDeparture'].apply(lambda date: (date - pd.to_datetime(\"1970-01-01\")).days)\n",
    "\n",
    "    # Encode holidays\n",
    "    us_holidays = holidays.US()\n",
    "    X_encoded.loc[:, 'is_holiday'] = [x in us_holidays for x in X_encoded['DateOfDeparture']]\n",
    "    X_encoded.loc[:, 'is_beginning_holidays'] = [(x not in us_holidays) & (x + datetime.timedelta(days=1) in us_holidays) for x in X_encoded['DateOfDeparture']]\n",
    "    X_encoded.loc[:, 'is_end_holidays'] = [(x in us_holidays) & (x + datetime.timedelta(days=1) not in us_holidays) for x in X_encoded['DateOfDeparture']]\n",
    "    \n",
    "    X_encoded.drop(columns=['DateOfDeparture'], inplace=True)\n",
    "\n",
    "    return X_encoded \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator():\n",
    "    # preprocessing the data\n",
    "    data_merger = FunctionTransformer(_merge_external_data)\n",
    "    date_encoder = FunctionTransformer(_encode_dates)\n",
    "\n",
    "    # create a preprocessor \n",
    "    preprocessor = make_pipeline(data_merger, date_encoder)\n",
    "\n",
    "    ###\n",
    "    target_encoder = TargetEncoder()\n",
    "    target_cols = ['connexion', 'event_level_dep_arr', 'day', 'weekday', 'week', 'year', 'month']\n",
    "\n",
    "\n",
    "    numerical_cols = ['WeeksToDeparture', 'std_wtd',\n",
    "                'departures_performed', 'passengers',\n",
    "                'utilization_rate', 'distance', 'gdp_capita_dep', 'population_dep',\n",
    "                'gdp_capita_arr', 'population_arr', 'sum_gdp_dep_arr',\n",
    "                'sum_pop_dep_arr', 'month_importance', 'connexion_importance']\n",
    "\n",
    "    numerical_encoder = make_pipeline(StandardScaler())#, fill_value=\"missing\"))\n",
    "\n",
    "    categorical_cols = ['Departure', 'Arrival']\n",
    "\n",
    "    categorical_encoder = make_pipeline(OrdinalEncoder())\n",
    "\n",
    "    encoder = make_column_transformer(\n",
    "        (categorical_encoder, categorical_cols),\n",
    "        (numerical_encoder, numerical_cols),\n",
    "        (target_encoder, target_cols),\n",
    "        remainder='passthrough'  # passthrough numerical columns as they are\n",
    "    )\n",
    "\n",
    "    # Models\n",
    "    xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42, reg_alpha=0.795, reg_lambda=0.172,\n",
    "                                colsample_bytree=0.836, gamma=0.042, learning_rate=0.114,\n",
    "                                max_depth=13, subsample=0.920, booster='dart')\n",
    "\n",
    "    #lgb_model = lgb.LGBMRegressor(objective='regression', random_state=42, metric=\"rmse\", learning_rate=0.114, max_depth=41, \n",
    "                                  #n_estimators=775, num_leaves=20, reg_lambda=0.123, reg_alpha=0.46, boosting='dart')\n",
    "    \n",
    "    lgb_model = lgb.LGBMRegressor(objective='regression', random_state=42, metric=\"rmse\", learning_rate=0.221, max_depth=10, \n",
    "                                  n_estimators=1931, num_leaves=28, reg_lambda=0.502, reg_alpha=0.121, boosting='dart')\n",
    "\n",
    "    hist_model = HistGradientBoostingRegressor(learning_rate=0.18, l2_regularization=0.511, max_depth=45)\n",
    "\n",
    "    ada_model = AdaBoostRegressor(n_estimators=493, learning_rate=0.011)\n",
    "    \n",
    "    cat_model = CatBoostRegressor(loss_function='RMSE', n_estimators=5000, learning_rate=0.05, max_depth=6, verbose=False)\n",
    "    \n",
    "    randf_model = RandomForestRegressor(n_estimators=296, max_depth=39, min_samples_split=3, min_samples_leaf=1 )\n",
    "    \n",
    "    gbr_model = GradientBoostingRegressor(learning_rate=0.111, max_depth=7, min_samples_leaf=2, min_samples_split=6,\n",
    "                                         n_estimators=217)\n",
    "    #sgd_model = SGDRegressor()\n",
    "\n",
    "    #estimators = [('xgb', xgb_model), ('lgb', lgb_model), ('hist', hist_model)]\n",
    "    estimators = [('xgb', xgb_model), ('lgb', lgb_model)]\n",
    "    \n",
    "    stacked_model = StackingRegressor(estimators=estimators,\n",
    "                                      final_estimator=LassoLarsCV(normalize=True)\n",
    "                                      )\n",
    "    \n",
    "    # create the final pipeline\n",
    "    return make_pipeline(preprocessor, encoder, stacked_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = get_estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.3118 +/- 0.0192\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scores = cross_val_score(\n",
    "    pipeline, X, y, cv=5, scoring='neg_mean_squared_error'\n",
    ")\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "\n",
    "print(\n",
    "    f\"RMSE: {np.mean(rmse_scores):.4f} +/- {np.std(rmse_scores):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing\n",
    "\n",
    "DART optimizer: \n",
    "LGB: RMSE: 0.3138 +/- 0.0198\n",
    "XGB: RMSE: 0.3363\n",
    "Stacked + DART LGB+XGB+HIST: RMSE: 0.3120 +/- 0.0195\n",
    "Stacked + DART LGB+XGB: RMSE: 0.3118 +/- 0.0192\n",
    "\n",
    "### + target encoding (connexion)\n",
    "Untuned:\n",
    "LGB: RMSE: 0.3292 +/- 0.0184\n",
    "XGB: RMSE: 0.3364 +/- 0.0192\n",
    "HIST: RMSE: 0.3472 +/- 0.0261\n",
    "Stacked, LGB+XGB+HIST: RMSE: 0.3229 +/- 0.0224\n",
    "\n",
    "Tuned:\n",
    "LGB: RMSE: 0.3292 +/- 0.0184\n",
    "XGB: RMSE: 0.3362 +/- 0.0192\n",
    "HIST: RMSE: 0.3441 +/- 0.0198\n",
    "Stacked, LGB+XGB+HIST: RMSE: 0.3230 +/- 0.0210\n",
    "\n",
    "### + monthly importance\n",
    "Untuned:\n",
    "LGB: RMSE: 0.3318 +/- 0.0205\n",
    "XGB: RMSE: 0.3443 +/- 0.0191\n",
    "HIST: RMSE: 0.3490 +/- 0.0205\n",
    "Stacked, LGB+XGB+HIST: 0.3274\n",
    "\n",
    "Tuned:\n",
    "LGB: RMSE: 0.3300 +/- 0.0195\n",
    "XGB: RMSE: 0.3419 +/- 0.0135\n",
    "HIST: RMSE: 0.3463 +/- 0.0179\n",
    "Stacked, LGB+XGB+HIST: RMSE: 0.3244 +/- 0.0177\n",
    "\n",
    "Ordinal encoding + no scaler:\n",
    "Stacked, LGB+XGB+HIST: RMSE: 0.3265 +/- 0.0186\n",
    "\n",
    "Ordinal encoding + standard scaler:\n",
    "Stacked, LGB+XGB+HIST: RMSE: 0.3265\n",
    "\n",
    "Standard scaler + one hot encoder: \n",
    "LGB: RMSE: 0.3397 +/- 0.0172\n",
    "Cat:\n",
    "Stacked, LGB+XGB+HIST:\n",
    "\n",
    "Standar scaler:\n",
    "LGB: RMSE: 0.3318 +/- 0.0205\n",
    "Stacked, LGB+XGB+HIST: RMSE: 0.3274 +/- 0.0203\n",
    "\n",
    "One hot encoder:\n",
    "LGB: RMSE: 0.3394 +/- 0.0181\n",
    "Stacked, LGB+XGB+HIST:\n",
    "\n",
    "\n",
    "RMSE: 0.3266 +/- 0.0189\n",
    "fill 0: RMSE: 0.3379 +/- 0.0254\n",
    "fill 1: RMSE: 0.3379 +/- 0.0253\n",
    "\n",
    "## Untuned:\n",
    "\n",
    "### +distance to holiday:\n",
    "Stacked, LGB+XGB+GBR: RMSE: 0.3279 +/- 0.0208\n",
    "Stacked, LGB+XGB+HIST: RMSE: 0.3286 +/- 0.0196\n",
    "LGB: RMSE: 0.3327 +/- 0.0185\n",
    "HIST: RMSE: 0.3443 +/- 0.0186\n",
    "XGB: RMSE: 0.3418 +/- 0.0221\n",
    "RANDF: RMSE: 0.4082 +/- 0.0248\n",
    "GBR: RMSE: 0.3422 +/- 0.0226\n",
    "\n",
    "### Final estimator: LassoLarsCV\n",
    "Stacked: RMSE: 0.3410 +/- 0.0268\n",
    "Randf: RMSE: 0.4349 +/- 0.0307\n",
    "Stacked, LGB+XGB+ADA: RMSE: 0.3385 +/- 0.0228\n",
    "Stacked, LGB+XGB+GBR: RMSE: 0.3380 +/- 0.0229\n",
    "Stacked, LGB+XGB+GBR+RANDF: RMSE: 0.3372 +/- 0.0223\n",
    "Stacked, LGB+XGB+GBR+ADA: RMSE: 0.3377 +/- 0.0227\n",
    "\n",
    "### Final estimator: ElasticNetCV\n",
    "Stacked, LGB+XGB+GBR: RMSE: 0.3460 +/- 0.0255\n",
    "Stacked, LGB+XGB+GBR+ADA: RMSE: 0.3445 +/- 0.0253\n",
    "\n",
    "### Final estimator: RidgeCV\n",
    "Stacked, LGB+XGB+GBR: RMSE: 0.3478 +/- 0.0258\n",
    "\n",
    "### Final estimator: Randf\n",
    "Stacked, LGB+XGB+GBR: RMSE: 0.3612 +/- 0.0193\n",
    "\n",
    "### Final estimator: LassoCV\n",
    "Stacked, LGB+XGB+GBR: RMSE: 0.3382 +/- 0.0231\n",
    "\n",
    "### Final estimator: LGB\n",
    "Stacked, LGB+XGB+GBR: RMSE: 0.3469 +/- 0.0234\n",
    "\n",
    "\n",
    "## Tuned:\n",
    "### +distance to holiday:\n",
    "Stacked, LGB+XGB+RANDF: RMSE: 0.3255 +/- 0.0196\n",
    "Stacked, LGB+XGB+GBR+RANDF: RMSE: 0.3232 +/- 0.0203\n",
    "Stacked, LGB+XGB+GBR: RMSE: 0.3255 +/- 0.0205\n",
    "Stacked, LGB+XGB+HIST: RMSE: 0.3261 +/- 0.0199\n",
    "HIST: RMSE: 0.3440 +/- 0.0183\n",
    "XGB: RMSE: 0.3418 +/- 0.0221\n",
    "LGB: RMSE: 0.3327 +/- 0.0185\n",
    "RANDF:RMSE: 0.4079 +/- 0.0243\n",
    "GBR: RMSE: 0.3422 +/- 0.0226\n",
    "\n",
    "### Final estimator: LassoLarsCV\n",
    "Stacked, LGB+XGB+GBR+RANDF: RMSE: 0.3348 +/- 0.0244\n",
    "Stacked, LGB+XGB+GBR: RMSE: 0.3374 +/- 0.0252\n",
    "Stacked, LGB+XGB+ADA+RANDF: RMSE: 0.3368 +/- 0.0222\n",
    "Stacked, LGB+XGB+ADA: RMSE: 0.3382 +/- 0.0227\n",
    "Stacked, LGB+XGB+RANDF: RMSE: 0.3381 +/- 0.0229\n",
    "Stacked, LGB+XGB: RMSE: 0.3392 +/- 0.0238\n",
    "Stacked, LGB+XGB+HIST+RANDF: RMSE: 0.3378 +/- 0.0229\n",
    "Stacked, LGB+XGB+HIST: RMSE: 0.3390 +/- 0.0238\n",
    "Stacked, LGB+HIST: RMSE: 0.3431 +/- 0.0227\n",
    "Stacked, XGB+HIST: RMSE: 0.3440 +/- 0.0245\n",
    "LGB: RMSE: 0.3441 +/- 0.0253\n",
    "XGB: RMSE: 0.3577 +/- 0.0257\n",
    "Hist: RMSE: 0.3565 +/- 0.0231\n",
    "Randf: RMSE: 0.4338 +/- 0.0307\n",
    "GBR: RMSE: 0.3540 +/- 0.0294"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGB\n",
    "lgb_model = lgb.LGBMRegressor(objective='regression', random_state=42, metric=\"rmse\", learning_rate=0.114, max_depth=41,n_estimators=775, num_leaves=20, reg_lambda=0.123, reg_alpha=0.46)\n",
    "\n",
    "### XGB\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42, reg_alpha=0.795, reg_lambda=0.172,\n",
    "                                colsample_bytree=0.836, gamma=0.042, learning_rate=0.114,\n",
    "                                max_depth=13, subsample=0.920)\n",
    "\n",
    "\n",
    "### HIST\n",
    "hist_model = HistGradientBoostingRegressor(learning_rate=0.18, l2_regularization=0.511, max_depth=45)\n",
    "\n",
    "\n",
    "### RANDF\n",
    "randf_model = RandomForestRegressor(n_estimators=296, max_depth=39, min_samples_split=3, min_samples_leaf=1 )\n",
    "\n",
    "### ADA\n",
    "ada_model = AdaBoostRegressor(n_estimators=493, learning_rate=0.011)\n",
    "\n",
    "### GBR\n",
    "gbr_model = GradientBoostingRegressor(learning_rate=0.111, max_depth=7, min_samples_leaf=2, min_samples_split=6,\n",
    "                                     n_estimators=217)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'pipeline', 'columntransformer', 'gradientboostingregressor', 'pipeline__memory', 'pipeline__steps', 'pipeline__verbose', 'pipeline__functiontransformer-1', 'pipeline__functiontransformer-2', 'pipeline__functiontransformer-1__accept_sparse', 'pipeline__functiontransformer-1__check_inverse', 'pipeline__functiontransformer-1__func', 'pipeline__functiontransformer-1__inv_kw_args', 'pipeline__functiontransformer-1__inverse_func', 'pipeline__functiontransformer-1__kw_args', 'pipeline__functiontransformer-1__validate', 'pipeline__functiontransformer-2__accept_sparse', 'pipeline__functiontransformer-2__check_inverse', 'pipeline__functiontransformer-2__func', 'pipeline__functiontransformer-2__inv_kw_args', 'pipeline__functiontransformer-2__inverse_func', 'pipeline__functiontransformer-2__kw_args', 'pipeline__functiontransformer-2__validate', 'columntransformer__n_jobs', 'columntransformer__remainder', 'columntransformer__sparse_threshold', 'columntransformer__transformer_weights', 'columntransformer__transformers', 'columntransformer__verbose', 'columntransformer__pipeline', 'columntransformer__pipeline__memory', 'columntransformer__pipeline__steps', 'columntransformer__pipeline__verbose', 'columntransformer__pipeline__simpleimputer', 'columntransformer__pipeline__ordinalencoder', 'columntransformer__pipeline__simpleimputer__add_indicator', 'columntransformer__pipeline__simpleimputer__copy', 'columntransformer__pipeline__simpleimputer__fill_value', 'columntransformer__pipeline__simpleimputer__missing_values', 'columntransformer__pipeline__simpleimputer__strategy', 'columntransformer__pipeline__simpleimputer__verbose', 'columntransformer__pipeline__ordinalencoder__categories', 'columntransformer__pipeline__ordinalencoder__dtype', 'gradientboostingregressor__alpha', 'gradientboostingregressor__ccp_alpha', 'gradientboostingregressor__criterion', 'gradientboostingregressor__init', 'gradientboostingregressor__learning_rate', 'gradientboostingregressor__loss', 'gradientboostingregressor__max_depth', 'gradientboostingregressor__max_features', 'gradientboostingregressor__max_leaf_nodes', 'gradientboostingregressor__min_impurity_decrease', 'gradientboostingregressor__min_impurity_split', 'gradientboostingregressor__min_samples_leaf', 'gradientboostingregressor__min_samples_split', 'gradientboostingregressor__min_weight_fraction_leaf', 'gradientboostingregressor__n_estimators', 'gradientboostingregressor__n_iter_no_change', 'gradientboostingregressor__presort', 'gradientboostingregressor__random_state', 'gradientboostingregressor__subsample', 'gradientboostingregressor__tol', 'gradientboostingregressor__validation_fraction', 'gradientboostingregressor__verbose', 'gradientboostingregressor__warm_start'])"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 27.0min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed: 50.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 63.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gradientboostingregressor__learning_rate': 0.11112267612279024,\n",
       " 'gradientboostingregressor__max_depth': 7,\n",
       " 'gradientboostingregressor__min_samples_leaf': 2,\n",
       " 'gradientboostingregressor__min_samples_split': 6,\n",
       " 'gradientboostingregressor__n_estimators': 217}"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GBR Hyper-paramter fine tuning (danger)\n",
    "'''\n",
    "params = {\n",
    "    'gradientboostingregressor__n_estimators': randint(50, 500),\n",
    "    'gradientboostingregressor__learning_rate': uniform(0.01, 1.0),\n",
    "    'gradientboostingregressor__max_depth': randint(1, 50),\n",
    "    'gradientboostingregressor__min_samples_split': randint(2, 10),\n",
    "    'gradientboostingregressor__min_samples_leaf': randint(1, 5)\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(pipeline, param_distributions=params, random_state=42, scoring='neg_root_mean_squared_error',\n",
    "                            n_iter=200, cv=5, verbose=1, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "search.fit(X, y)\n",
    "\n",
    "search.best_params_\n",
    "\n",
    "#report_best_scores(search.cv_results_, 1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   50.6s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 17.0min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 20.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'adaboostregressor__learning_rate': 0.010778765841014329,\n",
       " 'adaboostregressor__n_estimators': 493}"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ada Hyper-paramter fine tuning (danger)\n",
    "'''\n",
    "params = {\n",
    "    'adaboostregressor__n_estimators': randint(50, 500),\n",
    "    'adaboostregressor__learning_rate': uniform(0.01, 1.0),\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(pipeline, param_distributions=params, random_state=42, scoring='neg_root_mean_squared_error',\n",
    "                            n_iter=100, cv=5, verbose=1, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "search.fit(X, y)\n",
    "\n",
    "search.best_params_\n",
    "\n",
    "#report_best_scores(search.cv_results_, 1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   40.2s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed: 20.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 26.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'randomforestregressor__max_depth': 39,\n",
       " 'randomforestregressor__min_samples_leaf': 1,\n",
       " 'randomforestregressor__min_samples_split': 3,\n",
       " 'randomforestregressor__n_estimators': 296}"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest Hyper-paramter fine tuning (danger)\n",
    "'''\n",
    "params = {\n",
    "    'randomforestregressor__n_estimators': randint(50, 500),\n",
    "    'randomforestregressor__max_depth': randint(1, 50),\n",
    "    'randomforestregressor__min_samples_split': randint(2, 10),\n",
    "    'randomforestregressor__min_samples_leaf': randint(1, 5)\n",
    "    \n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(pipeline, param_distributions=params, random_state=42, scoring='neg_root_mean_squared_error',\n",
    "                            n_iter=200, cv=5, verbose=1, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "search.fit(X, y)\n",
    "\n",
    "search.best_params_\n",
    "\n",
    "#report_best_scores(search.cv_results_, 1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed: 34.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 45.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lgbmregressor__learning_rate': 0.22152530928899797,\n",
       " 'lgbmregressor__max_depth': 10,\n",
       " 'lgbmregressor__n_estimators': 1931,\n",
       " 'lgbmregressor__num_leaves': 28,\n",
       " 'lgbmregressor__reg_alpha': 0.12119748230615134,\n",
       " 'lgbmregressor__reg_lambda': 0.5026251042908592}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LGBM Hyper-paramter fine tuning (danger)\n",
    "#'''\n",
    "params = {\n",
    "    'lgbmregressor__learning_rate': uniform(0.01, 0.3),\n",
    "    'lgbmregressor__num_leaves': randint(10, 200),\n",
    "    'lgbmregressor__n_estimators': randint(50, 2000),\n",
    "    'lgbmregressor__max_depth': randint(1, 50),\n",
    "    'lgbmregressor__reg_lambda':  uniform(0.01, 1.0),\n",
    "    'lgbmregressor__reg_alpha':  uniform(0.01, 1.0)\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(pipeline, param_distributions=params, random_state=42, scoring='neg_root_mean_squared_error',\n",
    "                            n_iter=200, cv=5, verbose=1, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "search.fit(X, y)\n",
    "\n",
    "search.best_params_\n",
    "\n",
    "#report_best_scores(search.cv_results_, 1)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   37.4s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed: 12.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 16.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'histgradientboostingregressor__l2_regularization': 0.5113423988609378,\n",
       " 'histgradientboostingregressor__learning_rate': 0.18045488840615986,\n",
       " 'histgradientboostingregressor__max_depth': 45}"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hist Finetuning\n",
    "#pipeline.get_params().keys()\n",
    "'''\n",
    "params = {\n",
    "    \"histgradientboostingregressor__learning_rate\": uniform(0.03, 0.3), # default 0.1 \n",
    "    \"histgradientboostingregressor__l2_regularization\": uniform(0, 1),\n",
    "    \"histgradientboostingregressor__max_depth\": randint(1, 100),\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(pipeline, param_distributions=params, random_state=42, n_iter=200, cv=5, verbose=1, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "search.fit(X, y)\n",
    "\n",
    "search.best_params_\n",
    "\n",
    "#report_best_scores(search.cv_results_, 1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   30.2s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 16.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'xgbregressor__colsample_bytree': 0.9401760097327247,\n",
       " 'xgbregressor__gamma': 0.2101502442448101,\n",
       " 'xgbregressor__learning_rate': 0.08024477467772093,\n",
       " 'xgbregressor__max_depth': 17,\n",
       " 'xgbregressor__reg_alpha': 0.961549974355327,\n",
       " 'xgbregressor__reg_lambda': 0.13421996672805936,\n",
       " 'xgbregressor__subsample': 0.9062213026850383}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGB Finetuning\n",
    "#'''\n",
    "params = {\n",
    "    \"xgbregressor__colsample_bytree\": uniform(0.7, 0.3),\n",
    "    \"xgbregressor__gamma\": uniform(0.01, 1.0),\n",
    "    \"xgbregressor__learning_rate\": uniform(0.03, 0.3), # default 0.1 \n",
    "    \"xgbregressor__max_depth\": randint(1, 20),\n",
    "    \"xgbregressor__subsample\": uniform(0.01, 1.0),\n",
    "    \"xgbregressor__reg_alpha\": uniform(0.01, 1.0),\n",
    "    \"xgbregressor__reg_lambda\": uniform(0.01, 1.0)\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(pipeline, param_distributions=params, random_state=42, n_iter=200, cv=5, verbose=1, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "search.fit(X, y)\n",
    "\n",
    "search.best_params_\n",
    "#report_best_scores(search.cv_results_, 1)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;178m\u001b[1mTesting Number of air passengers prediction\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mReading train and test files from ./data ...\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mReading cv ...\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mTraining submissions/test_final ...\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mCV fold 0\u001b[0m\n",
      "/opt/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "\t\u001b[38;5;178m\u001b[1mscore   rmse       time\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m  \u001b[38;5;10m\u001b[1m0.097\u001b[0m  \u001b[38;5;150m39.440579\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.354\u001b[0m   \u001b[38;5;105m1.062911\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.380\u001b[0m   \u001b[38;5;218m0.601232\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mCV fold 1\u001b[0m\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "\t\u001b[38;5;178m\u001b[1mscore   rmse       time\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m  \u001b[38;5;10m\u001b[1m0.098\u001b[0m  \u001b[38;5;150m38.706069\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.357\u001b[0m   \u001b[38;5;105m1.124859\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.390\u001b[0m   \u001b[38;5;218m0.664597\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mCV fold 2\u001b[0m\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "\t\u001b[38;5;178m\u001b[1mscore   rmse       time\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m  \u001b[38;5;10m\u001b[1m0.098\u001b[0m  \u001b[38;5;150m38.827810\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.363\u001b[0m   \u001b[38;5;105m1.433213\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.380\u001b[0m   \u001b[38;5;218m0.782408\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mCV fold 3\u001b[0m\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "\t\u001b[38;5;178m\u001b[1mscore   rmse       time\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m  \u001b[38;5;10m\u001b[1m0.098\u001b[0m  \u001b[38;5;150m37.141560\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.377\u001b[0m   \u001b[38;5;105m1.128843\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.390\u001b[0m   \u001b[38;5;218m0.624964\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mCV fold 4\u001b[0m\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "\t\u001b[38;5;178m\u001b[1mscore   rmse       time\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m  \u001b[38;5;10m\u001b[1m0.099\u001b[0m  \u001b[38;5;150m37.069251\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.344\u001b[0m   \u001b[38;5;105m1.145411\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.370\u001b[0m   \u001b[38;5;218m0.656110\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mCV fold 5\u001b[0m\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "\t\u001b[38;5;178m\u001b[1mscore   rmse       time\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m  \u001b[38;5;10m\u001b[1m0.098\u001b[0m  \u001b[38;5;150m36.388919\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.344\u001b[0m   \u001b[38;5;105m1.188316\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.367\u001b[0m   \u001b[38;5;218m0.682922\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mCV fold 6\u001b[0m\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "\t\u001b[38;5;178m\u001b[1mscore   rmse       time\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m  \u001b[38;5;10m\u001b[1m0.097\u001b[0m  \u001b[38;5;150m36.121428\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.351\u001b[0m   \u001b[38;5;105m1.146549\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.364\u001b[0m   \u001b[38;5;218m0.648393\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mCV fold 7\u001b[0m\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "submissions/test_final/estimator.py:68: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
      "\t\u001b[38;5;178m\u001b[1mscore   rmse       time\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m  \u001b[38;5;10m\u001b[1m0.093\u001b[0m  \u001b[38;5;150m36.110734\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.353\u001b[0m   \u001b[38;5;105m1.206992\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.385\u001b[0m   \u001b[38;5;218m0.663918\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1m----------------------------\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mMean CV scores\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1m----------------------------\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore            rmse         time\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m  \u001b[38;5;10m\u001b[1m0.097\u001b[0mÂ \u001b[38;5;150m\u001b[38;5;150mÂ±\u001b[0m\u001b[0mÂ \u001b[38;5;150m0.0016\u001b[0m  \u001b[38;5;150m37.5\u001b[0mÂ \u001b[38;5;150m\u001b[38;5;150mÂ±\u001b[0m\u001b[0mÂ \u001b[38;5;150m1.24\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m    \u001b[38;5;12m\u001b[1m0.355\u001b[0mÂ \u001b[38;5;105m\u001b[38;5;105mÂ±\u001b[0m\u001b[0mÂ \u001b[38;5;105m0.01\u001b[0m    \u001b[38;5;105m1.2\u001b[0mÂ \u001b[38;5;105m\u001b[38;5;105mÂ±\u001b[0m\u001b[0mÂ \u001b[38;5;105m0.1\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.378\u001b[0mÂ \u001b[38;5;218m\u001b[38;5;218mÂ±\u001b[0m\u001b[0mÂ \u001b[38;5;218m0.0095\u001b[0m   \u001b[38;5;218m0.7\u001b[0mÂ \u001b[38;5;218m\u001b[38;5;218mÂ±\u001b[0m\u001b[0mÂ \u001b[38;5;218m0.05\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1m----------------------------\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mBagged scores\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1m----------------------------\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore   rmse\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.337\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.359\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!ramp-test --submission test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
